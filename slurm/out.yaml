---
# Source: slurm/charts/mysql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: slurm-mysql
  namespace: "tenant-sta-asenetar5-1"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.1
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/managed-by: Helm
  annotations:
automountServiceAccountToken: true
secrets:
  - name: slurm-mysql
---
# Source: slurm/templates/secrets/secret-job.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: slurm-secret-job
    app.kubernetes.io/instance: slurm-secret-job
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-secret-job
---
# Source: slurm/templates/syncer/syncer-account-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: slurm-syncer
    app.kubernetes.io/instance: slurm-syncer
    app.kubernetes.io/component: syncer
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-syncer
---
# Source: slurm/templates/accounting/accounting-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: slurm-mysql
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  mysql-password: OUJOMWV3dkx5Qk5xMzRYRUZqalZuSVdWNTdxY2ZoNXA=
  mysql-root-password: djVQU1hBckdsakt0TFpmUTNjYXhSMk50UktxQkhLb1Y=
---
# Source: slurm/templates/secrets/jwt-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: slurm-jwt-secret
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  jwt.key: "Xi5zQUxiL1Q5QlAsS1I1MiklfkE9bjMwQ2w5X1giYEU="
---
# Source: slurm/templates/secrets/munge-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: slurm-munge-secret
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  munge.key: "eVFGZHhFUlp5cWVxS0VzWGloMExHaWdUUTlmeGVtMTZBUjEyVU1HYUI0dW1meEtsOVdoWWd1allHMDNrVng5YVVFN2hvRFJic3lFek9QMExVS3FqUnBhdlVmd0VTdmFUQmJLUXhzYTlISkhtV1Jyaks0aVZBQ3d4eVRJSUtWQ3p5dVY4SGlpT0laM0dBQzE2aWFwdHJ0YTBQWlhFSVRxTTg2bFM5cnhDYnh6aVFSbnRsWk9LWGpubzdIWVpDNFFiUld1dHFkak43SHhpUUxxeWZwSkpFYnJtMGRERkZjWTdnaXdWOWx3M0hjV1R5bkZsU256ZElDODBSTmtwd1RvWWgzekdocVNtREVwVG9hV1VvNUFYdDhuUzcyOGhqU2JjVVpTSkVVNVl6R2p3WTRUMnpZNDNEdHpteU1BVEZWc1NBSFlGcU9mQW50ajZSUjdVU0k0SEtOUlREQzkwcTB2Z0FMdHVNbEhpM0prTmlidEVkZGVhclFrUkR3QUp6VVlJZTJGeUVzbDVQV1cwWkh0eXVKMHp0MXlwNGZMM2hWSkxOMGc0V3QxcWozdEdWSFZ0WEdLZGpGZnFGeXlVeW1Wa1JzZGQyTXZwdTNXUlFLc3l6R2xDb25nVThTTnNNYWd2MUJiMWZ1SHJOUnRwWndsTWp4ZHZCY2p4TGUyVXZQRFpJNHoxQjhvMVVkSmlYWmdON2RkSUM1Wlp1Z3JwMHV5N2lYeUtyTm5iRDBNVVlKYVBUVXZ2akJFS2tLZWd2UlNzSVRuTWdmbnFZNUhXNTFrbXRsN0lhR05TUnd6OEdmOW5tanpnZThxS29QdWdoQlFkdEZ0STR1T0xKczhMSFN2Z293dXl3am4yV01xc2pYd2xMVE1qMEI5RHU1elA2REQzWTRMc0ZuZFNBcmZhRVhvN0lHSG5lYm5ESXhSWU5oRUZXSVBOV2Q5S3NTam5WNERKYXdYZGNZNVJLdFkxWXhIa1M2UHQ1NnZGVTdTRHNuT2hEN1pOMjRJdklnQVhScFRwbFM4WDNTR1V1MTg3bFNublV2dDlZc1J4NFBKanMzeVpZbjVlSUlwYmhsZDR2dzZXZDJrcXdtUkszS2pwZTNhRDNsOVNuR09YR3pHdlNJbnBYZE9WVlFJRkowamliVUVyY1owR0FCMVZZSHR1OTN0cHBVazJCS2JkZFRiQXBhTzZ6Q3VIUkRlMk93Y0tpMFpTSjdMc3BMU3E4MGhqZDZMbXBNcW1GNjNRMFJKY3JwazZ6RVRLMmU4SXE0dkFhRmxCcllld25tTHF4VVJyN3gzZ2I0VEY1YWJ5aUFaaDNtUTJZbnFleXpKcFR6OGRlenlYYmlqeUQzREF2Tzh1OGJHRA=="
---
# Source: slurm/templates/secrets/sudoers-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: slurm-slurm-sudoers
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
type: Opaque
stringData:
  10-ldap-sudoers: |-
    %admin ALL=(ALL) NOPASSWD: ALL
---
# Source: slurm/charts/mysql/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slurm-mysql
  namespace: "tenant-sta-asenetar5-1"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.1
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    [mysqld]
    default_authentication_plugin=mysql_native_password
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mysql
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    datadir=/bitnami/mysql/data
    tmpdir=/opt/bitnami/mysql/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    log-error=/opt/bitnami/mysql/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    slow_query_log_file=/opt/bitnami/mysql/logs/mysqld.log
    long_query_time=10.0
    
    [client]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
---
# Source: slurm/templates/config/krb5-conf-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slurm-krb5-conf
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
data:
  krb5.conf: |
    [libdefaults]
    default_realm = "it.doesnt.even.matter"
    rdns = false
---
# Source: slurm/templates/config/slurm-conf-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slurm-slurm-conf
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
data:
  slurm.conf: |
    ClusterName=slurm
    SlurmctldHost=slurm-controller
    SlurmctldParameters=cloud_dns,idle_on_node_suspend,node_reg_mem_percent=95
    CommunicationParameters=NoAddrCache,KeepAliveTime=60,keepaliveinterval=10,keepaliveprobes=3
    SlurmctldTimeout=60
    SlurmdTimeout=30
    SuspendTime=INFINITE
    PrivateData=cloud
    MpiDefault=pmi2 # pmix can be used for Ubuntu 22
    ProctrackType=proctrack/linuxproc
    ReturnToService=2
    SlurmctldPidFile=/var/run/slurmctld.pid
    SlurmctldPort=6817
    SlurmdPidFile=/var/run/slurmd.pid
    SlurmdPort=6818
    SlurmdSpoolDir=/var/spool/slurmd
    SlurmUser=slurm
    StateSaveLocation=/var/spool/slurmctld/save
    SwitchType=switch/none
    TaskPlugin=task/none
    #TaskPluginParam=None
    InactiveLimit=0
    KillWait=30
    MinJobAge=300
    Waittime=0
    SchedulerType=sched/backfill
    SelectType=select/cons_tres
    SelectTypeParameters=CR_Core
    JobCompType=jobcomp/none
    SlurmctldDebug=verbose
    SlurmctldLogFile=/dev/null
    SlurmSchedLogLevel=1
    SlurmSchedLogFile=/dev/null
    SlurmdDebug=debug3
    SlurmdLogFile=/dev/null
    TreeWidth=65533
    GresTypes=gpu
    MaxNodeCount=1024
    DefMemPerCPU=4096
    UnkillableStepTimeout=900
    RebootProgram=/usr/share/sunk/bin/reboot.sh
    JobAcctGatherType=jobacct_gather/linux
    JobAcctGatherFrequency=30
    AccountingStorageEnforce=qos,limits
    AccountingStorageTRES=gres/gpu
    AccountingStorageType=accounting_storage/slurmdbd
    AccountingStorageHost=slurm-accounting
    AccountingStorageUser=coreweave
    AccountingStoragePort=6819
    PrologFlags=Alloc,Serial

    AuthAltTypes=auth/jwt
    AuthAltParameters=jwt_key=/etc/jwt/jwt_hs256.key
    PreemptType=preempt/partition_prio
    PreemptMode=REQUEUE
    PreemptExemptTime=-1
    JobRequeue=1
    PriorityType=priority/multifactor
    PriorityDecayHalfLife=7-0
    PriorityCalcPeriod=5
    PriorityFavorSmall=NO
    PriorityMaxAge=7-0
    PriorityUsageResetPeriod=WEEKLY
    PriorityWeightAge=10000
    PriorityWeightFairshare=20000
    PriorityWeightPartition=50000
    PrivateData=jobs
    SchedulerParameters=nohold_on_prolog_fail
    SlurmctldDebug=debug
    Nodeset=rtx4000-cu117 Feature=rtx4000-cu117
    PartitionName=rtx4000-cu117 Nodes=rtx4000-cu117 Default=no MaxTime=INFINITE State=UP
    include /etc/slurm/slurm-rtx4000-cu117-nodes.conf
    
    PartitionName=all Nodes=ALL Default=YES MaxTime=INFINITE State=UP
    

  gres.conf: |
    AutoDetect=off
    
    NodeName=slurm-rtx4000-cu117-000-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-001-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-002-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-003-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-004-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-005-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-006-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-007-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-008-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-009-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-010-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-011-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-012-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-013-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-014-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-015-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-016-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-017-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-018-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-019-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-020-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-021-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-022-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-023-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-024-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-025-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-026-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-027-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-028-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-029-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-030-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-031-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-032-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-033-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-034-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-035-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-036-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-037-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-038-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-039-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-040-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-041-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-042-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-043-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-044-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-045-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-046-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-047-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-048-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-049-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-050-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-051-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-052-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-053-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-054-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-055-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-056-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-057-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-058-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-059-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-060-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-061-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-062-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-063-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-064-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-065-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-066-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-067-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-068-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-069-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-070-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-071-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-072-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-073-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-074-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-075-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-076-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-077-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-078-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-079-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-080-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-081-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-082-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-083-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-084-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-085-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-086-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-087-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-088-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-089-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-090-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-091-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-092-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-093-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-094-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-095-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-096-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-097-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-098-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-099-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-100-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-101-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-102-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-103-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-104-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-105-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-106-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-107-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-108-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-109-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-110-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-111-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-112-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-113-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-114-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-115-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-116-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-117-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-118-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-119-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-120-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-121-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-122-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-123-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-124-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-125-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-126-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-127-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-128-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-129-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-130-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-131-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-132-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-133-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-134-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-135-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-136-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-137-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-138-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-139-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-140-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-141-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-142-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-143-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-144-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-145-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-146-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-147-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-148-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-149-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-150-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-151-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-152-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-153-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-154-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-155-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-156-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-157-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-158-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-159-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-160-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-161-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-162-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-163-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-164-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-165-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-166-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-167-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-168-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-169-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-170-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-171-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-172-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-173-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-174-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-175-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-176-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-177-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-178-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-179-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-180-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-181-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-182-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-183-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-184-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-185-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-186-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-187-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-188-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-189-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-190-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-191-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-192-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-193-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-194-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-195-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-196-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-197-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-198-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-199-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-200-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-201-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-202-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-203-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-204-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-205-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-206-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-207-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-208-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-209-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-210-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-211-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-212-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-213-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-214-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-215-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-216-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-217-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-218-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-219-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-220-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-221-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-222-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-223-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-224-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-225-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-226-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-227-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-228-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-229-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-230-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-231-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-232-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-233-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-234-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-235-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-236-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-237-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-238-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-239-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
    NodeName=slurm-rtx4000-cu117-240-[00-99] Name=gpu Type=quadro_rtx_4000 File=/dev/nvidia[0-6]
---
# Source: slurm/templates/config/slurmdbd-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slurm-slurmdbd-conf
data:
  slurmdbd.conf: |
    ArchiveEvents=yes
    ArchiveJobs=yes
    ArchiveResvs=yes
    ArchiveSteps=no
    ArchiveSuspend=no
    ArchiveTXN=no
    ArchiveUsage=no
    AuthType=auth/munge
    DbdHost=slurm-accounting
    DbdPort=6819
    DebugLevel=verbose
    PurgeEventAfter=1month
    PurgeJobAfter=12month
    PurgeResvAfter=1month
    PurgeStepAfter=1month
    PurgeSuspendAfter=1month
    PurgeTXNAfter=12month
    PurgeUsageAfter=24month
    SlurmUser=slurm
    LogFile=/dev/null
    PidFile=/var/run/slurmdbd.pid
    StorageLoc=slurm_acct_db
    StorageType=accounting_storage/mysql
    StorageUser=coreweave
    StoragePass=$MYSQL_PASSWORD
    StorageHost=slurm-mysql
    StoragePort=3306
    AuthAltTypes=auth/jwt
    AuthAltParameters=jwt_key=/etc/jwt/jwt_hs256.key
---
# Source: slurm/templates/config/sssd-conf-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slurm-sssd-conf
data:
  sssd.conf: |
    [domain/default]
    cache_credentials = True
    debug_level = 512
    default_shell = 
    fallback_homedir = /mnt/nvme/home/%u
    id_provider = ldap
    ldap_default_authtok = 22HFrai3LmRX
    ldap_default_bind_dn = cn=admin,dc=coreweave,dc=cloud
    ldap_search_base = dc=coreweave,dc=cloud
    ldap_tls_reqcert = allow
    ldap_uri = ldap://openldap
    use_fully_qualified_names = False
    # Enumerate all entries to get generate full passwd file
    enumerate = true
    ignore_group_members = True
    ldap_purge_cache_timeout = 0
    krb5_auth_timeout = 10
    ldap_schema = rfc2307
    auth_provider = ldap
    ldap_access_order = filter
    ldap_access_filter = (objectClass=posixAccount)
    
    
    [sssd]
    debug_level = 512
    config_file_version = 2
    services = nss, pam, ssh
    domains = default
    [nss]
    debug_level = 512
    filter_users = nobody,root
    filter_groups = nobody,root
    entry_negative_timeout = 600
    [pam]
    debug_level = 512
    offline_credentials_expiration = 7
    pam_id_timeout = 10
---
# Source: slurm/templates/pvcs/slurmctld-state-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: slurm-slurmctld-state
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  storageClassName: block-nvme-lga1
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 32Gi
---
# Source: slurm/templates/pvcs/ssh-host-keys-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: slurm-ssh-host-keys
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  storageClassName: shared-nvme-lga1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
# Source: slurm/templates/secrets/secret-job.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: slurm-secret-job-role
    app.kubernetes.io/instance: slurm-secret-job
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-secret-job-role
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - create
---
# Source: slurm/templates/syncer/syncer-account-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: slurm-syncer-role
    app.kubernetes.io/instance: slurm-syncer
    app.kubernetes.io/component: syncer
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-syncer-role
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - pods/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - sunk.coreweave.com
  resources:
  - nodesets
  verbs:
  - get
  - list
  - watch
---
# Source: slurm/templates/secrets/secret-job.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: slurm-secret-job-rolebinding
    app.kubernetes.io/instance: slurm-secret-job
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-secret-job-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: slurm-secret-job-role
subjects:
- kind: ServiceAccount
  name: slurm-secret-job
  namespace: tenant-sta-asenetar5-1
---
# Source: slurm/templates/syncer/syncer-account-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: slurm-syncer-rolebinding
    app.kubernetes.io/instance: slurm-syncer
    app.kubernetes.io/component: syncer
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-syncer-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: slurm-syncer-role
subjects:
- kind: ServiceAccount
  name: slurm-syncer
---
# Source: slurm/charts/mysql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-mysql-headless
  namespace: "tenant-sta-asenetar5-1"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.1
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mysql
      port: 3306
      targetPort: mysql
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/component: primary
---
# Source: slurm/charts/mysql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-mysql
  namespace: "tenant-sta-asenetar5-1"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.1
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: null
  selector: 
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/component: primary
---
# Source: slurm/templates/accounting/accounting-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-accounting
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: slurmdbd
    app.kubernetes.io/instance: slurm-accounting
    app.kubernetes.io/component: "accounting"
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  ports:
  - name: slurmdbd
    protocol: TCP
    port: 6819
    targetPort: 6819
---
# Source: slurm/templates/compute/compute-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-compute
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
  - name: sshd
    port: 22
    protocol: TCP
    targetPort: sshd
  selector:
    app.kubernetes.io/name: slurmd
    app.kubernetes.io/component: "compute"
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
---
# Source: slurm/templates/controller/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-controller
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: slurmctld
    app.kubernetes.io/instance: slurm-controller
    app.kubernetes.io/component: "controller"
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  ports:
  - name: slurmctld
    protocol: TCP
    port: 6817
    targetPort: 6817
---
# Source: slurm/templates/login/login-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/hostname: slurm-login.tenant-sta-asenetar5-1.coreweave.cloud
  name: slurm-login
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  ports:
  - name: ssh
    port: 22
    targetPort: sshd
    protocol: TCP
  selector:
    app.kubernetes.io/name: login
    app.kubernetes.io/instance: slurm-login
---
# Source: slurm/templates/rest/rest-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-rest
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: slurmrestd
    app.kubernetes.io/instance: slurm-rest
    app.kubernetes.io/component: "rest"
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  ports:
  - name: slurmrestd
    protocol: TCP
    port: 6819
    targetPort: 6819
---
# Source: slurm/templates/syncer/syncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: slurm-syncer
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  selector:
    app.kubernetes.io/name: slurm-syncer
    app.kubernetes.io/instance: slurm-syncer
    app.kubernetes.io/component: "syncer"
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  ports:
  - name: hooksapi
    protocol: TCP
    port: 8000
    targetPort: 8000
---
# Source: slurm/templates/accounting/accounting-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slurm-accounting
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: slurmdbd
      app.kubernetes.io/instance: slurm-accounting
      app.kubernetes.io/component: "accounting"
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: slurmdbd
        app.kubernetes.io/instance: slurm-accounting
        app.kubernetes.io/component: "accounting"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "slurmdbd"
        vpc.coreweave.cloud/kubernetes-networking: "true"
    spec:
      terminationGracePeriodSeconds: 30
      hostname: slurm-accounting
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        env:
        - name: ROLE
          value: "accounting"
        - name: SLURM_USER
          value: "401"
        - name: SLURM_GROUP
          value: "401"
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: slurm-mysql
              key: mysql-password
        volumeMounts:
        - name: sssd-pipes
          mountPath: /runtime/var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
        - name: slurmdbd-conf
          mountPath: /slurm
      containers:
      
      - name: sssd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["sssd", "-i"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: krb5-conf
          mountPath: /etc/krb5.conf
          subPath: krb5.conf
        - name: sssd-conf
          mountPath: /etc/sssd/sssd.conf
          subPath: sssd.conf
      # The user lookup is needed to get AD SIDs mapped automatically
      # See: https://www.mail-archive.com/sssd-users@lists.fedorahosted.org/msg07533.html
      - name: user-lookup
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
          - "bash"
          - "-c"
          - |
            while true; do
              getent passwd psalanki
              sleep 5
            done;
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: run
          mountPath: /run
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: slurmdbd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["slurmdbd", "-D"]
        resources:
          limits:
            cpu: 4
            memory: 16Gi
          requests:
            cpu: 4
            memory: 16Gi
        ports:
        - containerPort: 6819
          name: slurmdbd
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: jwt-secret
          mountPath: /etc/jwt/jwt_hs256.key
          subPath: jwt.key
          readOnly: true
        livenessProbe:
          tcpSocket:
            port: 6819
          initialDelaySeconds: 15
          periodSeconds: 10
          failureThreshold: 5
          successThreshold: 1
        securityContext:
          runAsNonRoot: true
          runAsUser: 401
          runAsGroup: 401
      volumes:
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      - name: jwt-secret
        secret:
          secretName: slurm-jwt-secret
      - name: slurm-conf
        configMap:
          name: slurm-slurm-conf
      - name: slurmdbd-conf
        configMap:
          name: slurm-slurmdbd-conf
          defaultMode: 0600
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
---
# Source: slurm/templates/controller/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slurm-controller
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: slurmctld
      app.kubernetes.io/instance: slurm-controller
      app.kubernetes.io/component: "controller"
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: slurmctld
        app.kubernetes.io/instance: slurm-controller
        app.kubernetes.io/component: "controller"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "slurmctld"
        vpc.coreweave.cloud/kubernetes-networking: "true"
    spec:
      terminationGracePeriodSeconds: 30
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      enableServiceLinks: false
      hostname: slurm-controller
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        env:
        - name: ROLE
          value: "controller"
        - name: SLURM_USER
          value: "401"
        - name: SLURM_GROUP
          value: "401"
        volumeMounts:
        - name: sssd-pipes
          mountPath: /runtime/var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
        - name: slurmctld-state
          mountPath: /var/spool/slurmctld
      containers:
      
      - name: sssd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["sssd", "-i"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: krb5-conf
          mountPath: /etc/krb5.conf
          subPath: krb5.conf
        - name: sssd-conf
          mountPath: /etc/sssd/sssd.conf
          subPath: sssd.conf
      # The user lookup is needed to get AD SIDs mapped automatically
      # See: https://www.mail-archive.com/sssd-users@lists.fedorahosted.org/msg07533.html
      - name: user-lookup
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
          - "bash"
          - "-c"
          - |
            while true; do
              getent passwd psalanki
              sleep 5
            done;
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: run
          mountPath: /run
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: watch
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
        - "/usr/share/sunk/bin/reconfigure-watch.sh"
        - "/etc/slurm/slurm.conf"
        - "/etc/slurm/gres.conf"
        - "/etc/slurm/slurm-rtx4000-cu117-nodes.conf"
        resources:
          limits:
            cpu: 250m
            memory: 100Mi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: slurm-conf
          mountPath: /etc/slurm
          readOnly: true
        securityContext:
          runAsNonRoot: true
          runAsUser: 401
          runAsGroup: 401
      - name: slurmctld
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["slurmctld", "-D"]
        resources:
          limits:
            cpu: 4
            memory: 16Gi
          requests:
            cpu: 4
            memory: 16Gi
        ports:
        - containerPort: 6817
          name: slurmctld
        livenessProbe:
          exec:
            command:
            - sinfo
          failureThreshold: 6
          initialDelaySeconds: 15
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 10
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: slurmctld-state
          mountPath: /var/spool/slurmctld
        - name: slurm-conf
          mountPath: /etc/slurm
          readOnly: true
        - name: jwt-secret
          mountPath: /etc/jwt/jwt_hs256.key
          subPath: jwt.key
          readOnly: true
        securityContext:
          runAsNonRoot: true
          runAsUser: 401
          runAsGroup: 401
      volumes:
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      - name: jwt-secret
        secret:
          secretName: slurm-jwt-secret
      - name: slurmctld-state
        persistentVolumeClaim:
          claimName: slurm-slurmctld-state
      - name: slurm-conf
        projected:
          sources:
            - configMap:
                name: slurm-slurm-conf
            - configMap:
                name: slurm-rtx4000-cu117-nodes-conf
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      tolerations:
        null
---
# Source: slurm/templates/metrics/metrics-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slurm-metrics
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-slurm-exporter
      app.kubernetes.io/instance: slurm-metrics
      app.kubernetes.io/component: "metrics"
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: prometheus-slurm-exporter
        app.kubernetes.io/instance: slurm-metrics
        app.kubernetes.io/component: "metrics"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "prometheus-slurm-exporter"
        vpc.coreweave.cloud/kubernetes-networking: "true"
    spec:
      terminationGracePeriodSeconds: 30
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      enableServiceLinks: false
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/prometheus-slurm-exporter:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        volumeMounts:
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
      containers:
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: exporter
        image: "registry.gitlab.com/coreweave/sunk/prometheus-slurm-exporter:v1.0.0"
        command: ["prometheus-slurm-exporter", "--gpus-acct", "-listen-address", "0.0.0.0:9341"]
        resources:
          limits:
            cpu: 1
            memory: 4Gi
          requests:
            cpu: 1
            memory: 4Gi
        ports:
        - containerPort: 9341
          name: metrics
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
      volumes:
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      tolerations:
        null
---
# Source: slurm/templates/rest/rest-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slurm-rest
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: slurmrestd
      app.kubernetes.io/instance: slurm-rest
      app.kubernetes.io/component: "rest"
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  replicas: 1 # by default is 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: slurmrestd
        app.kubernetes.io/instance: slurm-rest
        app.kubernetes.io/component: "rest"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "slurmrestd"
        vpc.coreweave.cloud/kubernetes-networking: "true"
    spec:
      terminationGracePeriodSeconds: 5
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      enableServiceLinks: false
      hostname: slurm-rest
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        env:
        - name: ROLE
          value: "rest"
        volumeMounts:
        - name: sssd-pipes
          mountPath: /runtime/var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
      containers:
      
      - name: sssd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["sssd", "-i"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: krb5-conf
          mountPath: /etc/krb5.conf
          subPath: krb5.conf
        - name: sssd-conf
          mountPath: /etc/sssd/sssd.conf
          subPath: sssd.conf
      # The user lookup is needed to get AD SIDs mapped automatically
      # See: https://www.mail-archive.com/sssd-users@lists.fedorahosted.org/msg07533.html
      - name: user-lookup
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
          - "bash"
          - "-c"
          - |
            while true; do
              getent passwd psalanki
              sleep 5
            done;
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: run
          mountPath: /run
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: slurmrestd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["slurmrestd", "-v", "-v", "0.0.0.0:6819"]
        ports:
        - containerPort: 6819
          name: slurmrestd
        env:
        - name: SLURMRESTD_SECURITY
          value: disable_unshare_sysv,disable_unshare_files
        - name: SLURM_JWT
          value: enabled
        resources:
          limits:
            cpu: 1
            memory: 2Gi
          requests:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: slurm-conf
          mountPath: /etc/slurm
          readOnly: true
        startupProbe:
          tcpSocket:
            port: slurmrestd
          failureThreshold: 20
          periodSeconds: 2
        livenessProbe:
          tcpSocket:
            port: slurmrestd
          failureThreshold: 2
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: slurmrestd
          periodSeconds: 5
          failureThreshold: 1
        securityContext:
          runAsNonRoot: true
          runAsUser: 401
          runAsGroup: 401
      volumes:
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      - name: slurm-conf
        projected:
          sources:
            - configMap:
                name: slurm-slurm-conf
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      tolerations:
        null
---
# Source: slurm/templates/syncer/syncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: slurm-syncer
    app.kubernetes.io/instance: slurm-syncer
    app.kubernetes.io/component: syncer
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-syncer
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: slurm-syncer
      app.kubernetes.io/instance: slurm-syncer
      app.kubernetes.io/component: syncer
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        app.kubernetes.io/name: slurm-syncer
        app.kubernetes.io/instance: slurm-syncer
        app.kubernetes.io/component: syncer
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      containers:
      - args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --hooks-api-bind-address=:8000
        - --components
        - syncer
        - --watch-namespace=tenant-sta-asenetar5-1
        - --slurm-auth-token=$(SLURM_TOKEN)
        - --slurm-api-base=http://slurm-rest:6819/
        env:
        - name: SLURM_TOKEN
          valueFrom:
            secretKeyRef:
              name: slurm-syncer
              key: slurm-token
        command:
        - /manager
        image: "registry.gitlab.com/coreweave/sunk/operator:v1.0.0"
        ports:
        - containerPort: 8000
          name: hooksapi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        name: syncer
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          limits:
            cpu: "2"
            memory: 500Mi
          requests:
            cpu: 200m
            memory: 500Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: slurm-syncer
      terminationGracePeriodSeconds: 10
---
# Source: slurm/charts/mysql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: slurm-mysql
  namespace: "tenant-sta-asenetar5-1"
  labels:
    app.kubernetes.io/name: mysql
    helm.sh/chart: mysql-9.4.1
    app.kubernetes.io/instance: slurm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  podManagementPolicy: ""
  selector:
    matchLabels: 
      app.kubernetes.io/name: mysql
      app.kubernetes.io/instance: slurm
      app.kubernetes.io/component: primary
  serviceName: slurm-mysql
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: b56411a9989c33e6bbbda07b0d9661269d2e5019fc3cde4a969e5ed03eec838f
      labels:
        app.kubernetes.io/name: mysql
        helm.sh/chart: mysql-9.4.1
        app.kubernetes.io/instance: slurm
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: slurm-mysql
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: mysql
          image: docker.io/bitnami/mysql:8.0.31-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: slurm-mysql
                  key: mysql-root-password
            - name: MYSQL_USER
              value: "coreweave"
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: slurm-mysql
                  key: mysql-password
            - name: MYSQL_DATABASE
              value: "slurm_acct_db"
          envFrom:
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          startupProbe:
            failureThreshold: 30
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/mysql
            - name: config
              mountPath: /opt/bitnami/mysql/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: slurm-mysql
  volumeClaimTemplates:
    - metadata:
        name: data
        labels: 
          app.kubernetes.io/name: mysql
          app.kubernetes.io/instance: slurm
          app.kubernetes.io/component: primary
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: block-nvme-lga1
---
# Source: slurm/templates/login/login-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: slurm-login
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: login
      app.kubernetes.io/instance: slurm-login
      app.kubernetes.io/component: "login"
      helm.sh/chart: slurm
      app.kubernetes.io/part-of: slurm
      app.kubernetes.io/managed-by: Helm
  serviceName: slurm-login
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: login
        app.kubernetes.io/instance: slurm-login
        app.kubernetes.io/component: "login"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "sshd"
        vpc.coreweave.cloud/kubernetes-networking: "true"
    spec:
      terminationGracePeriodSeconds: 30
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      enableServiceLinks: false
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/controller-extras:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        env:
        - name: ROLE
          value: "login"
        volumeMounts:
        - name: sssd-pipes
          mountPath: /runtime/var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
        - name: ssh-host-keys
          mountPath: /opt/sunk/etc/ssh
      containers:
      
      - name: sssd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["sssd", "-i"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: krb5-conf
          mountPath: /etc/krb5.conf
          subPath: krb5.conf
        - name: sssd-conf
          mountPath: /etc/sssd/sssd.conf
          subPath: sssd.conf
      # The user lookup is needed to get AD SIDs mapped automatically
      # See: https://www.mail-archive.com/sssd-users@lists.fedorahosted.org/msg07533.html
      - name: user-lookup
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
          - "bash"
          - "-c"
          - |
            while true; do
              getent passwd psalanki
              sleep 5
            done;
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: run
          mountPath: /run
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: sshd
        image: "registry.gitlab.com/coreweave/sunk/controller-extras:v1.0.0"
        resources:
          limits:
            cpu: 4
            memory: 8Gi
          requests:
            cpu: 4
            memory: 8Gi
        command: ["/usr/bin/tini", "--"]
        args: ["/usr/sbin/sshd", "-D"]
        ports:
        - containerPort: 22
          name: sshd
        volumeMounts:
        - name: sudoers-groups
          mountPath: /etc/sudoers.d/10-ldap-sudoers
          subPath: 10-ldap-sudoers
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - mountPath: /dev/shm
          name: dshm
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: ssh-host-keys
          mountPath: /opt/sunk/etc/ssh
        - name: slurm-conf
          mountPath: /etc/slurm
          readOnly: true
        - name: data-nvme
          mountPath: /mnt/nvme
        - name: data-hdd
          mountPath: /mnt/hdd
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      - name: slurm-conf
        projected:
          sources:
            - configMap:
                name: slurm-slurm-conf
            - configMap:
                name: slurm-rtx4000-cu117-nodes-conf
      - name: data-nvme
        persistentVolumeClaim:
          claimName: data-nvme
      - name: data-hdd
        persistentVolumeClaim:
          claimName: data-hdd
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      - name: ssh-host-keys
        persistentVolumeClaim:
          claimName: slurm-ssh-host-keys
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      tolerations:
        null
---
# Source: slurm/templates/compute/compute-nodeset.yaml
apiVersion: sunk.coreweave.com/v1alpha1
kind: NodeSet
metadata:
  name: slurm-rtx4000-cu117
  labels:
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  annotations:
    slurm.coreweave.com/cpu: "8"
    slurm.coreweave.com/mem: "16Gi"
    slurm.coreweave.com/features: rtx4000-cu117,cu117
    slurm.coreweave.com/gres: Gres=gpu:quadro_rtx_4000:7
spec:
  serviceName: slurm-compute
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25
  replicas: 2
  template:
    metadata:
      labels:
        app.kubernetes.io/name: slurmd
        app.kubernetes.io/instance: slurm-rtx4000-cu117
        app.kubernetes.io/component: "compute"
        helm.sh/chart: slurm
        app.kubernetes.io/part-of: slurm
        app.kubernetes.io/managed-by: Helm
      annotations:
        kubectl.kubernetes.io/default-container: "slurmd"
        vpc.coreweave.cloud/kubernetes-networking: "true"

    spec:
      terminationGracePeriodSeconds: 20
      enableServiceLinks: false
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      # Munge key does not need to go in the global etc, could go in a specific folder just for munge
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/slurmd-cw-cu117-extras:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        env:
        - name: ROLE
          value: "compute"
        volumeMounts:
        - name: sssd-pipes
          mountPath: /runtime/var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: tmp
          mountPath: /tmp
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
        - name: opt-sunk
          mountPath: /opt/sunk
      containers:
      
      - name: sssd
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["sssd", "-i"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: krb5-conf
          mountPath: /etc/krb5.conf
          subPath: krb5.conf
        - name: sssd-conf
          mountPath: /etc/sssd/sssd.conf
          subPath: sssd.conf
      # The user lookup is needed to get AD SIDs mapped automatically
      # See: https://www.mail-archive.com/sssd-users@lists.fedorahosted.org/msg07533.html
      - name: user-lookup
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command:
          - "bash"
          - "-c"
          - |
            while true; do
              getent passwd psalanki
              sleep 5
            done;
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: etc
          mountPath: /etc
        - name: run
          mountPath: /run
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: slurmd
        image: "registry.gitlab.com/coreweave/sunk/slurmd-cw-cu117-extras:v1.0.0"
        resources:
          limits:
            memory: 16Gi
            sunk.coreweave.com/accelerator: 7
          requests:
            cpu: 8
            memory: 16Gi
            sunk.coreweave.com/accelerator: 7
        args:
        - "slurmd"
        - "-c"
        - "-D"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 6818
          name: slurmd
          protocol: TCP
        volumeMounts:
        
        - name: sssd-pipes
          mountPath: /var/lib/sss/pipes
        - name: sssd-cache
          mountPath: /var/lib/sss/db
        - name: sudoers-groups
          mountPath: /etc/sudoers.d/10-ldap-sudoers
          subPath: 10-ldap-sudoers
        - name: opt-sunk
          mountPath: /opt/sunk
        - mountPath: /dev/shm
          name: dshm
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        - name: tmp
          mountPath: /tmp
        - name: slurm-conf
          mountPath: /etc/slurm
          readOnly: true
        - name: data-nvme
          mountPath: /mnt/nvme
        - name: data-hdd
          mountPath: /mnt/hdd
      volumes:
      - name: slurm-conf
        projected:
          sources:
            - configMap:
                name: slurm-slurm-conf
            - configMap:
                name: slurm-rtx4000-cu117-nodes-conf
      
      
      - name: dshm
        emptyDir:
          medium: Memory
      - name: opt-sunk
        emptyDir: {}
      - name: tmp
        emptyDir: {}
      - name: data-nvme
        persistentVolumeClaim:
          claimName: data-nvme
      - name: data-hdd
        persistentVolumeClaim:
          claimName: data-hdd
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      automountServiceAccountToken: false
      priorityClassName: normal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LAS1
              - key: gpu.nvidia.com/model
                operator: In
                values:
                - Quadro_RTX_4000
              - key: node.coreweave.cloud/reserved
                operator: In
                values:
                - 3b3525672b1d07bb0e82ed6aa93fb7b63151b984
      tolerations:
        - effect: NoSchedule
          key: node.coreweave.cloud/reserved
          operator: Equal
          value: 3b3525672b1d07bb0e82ed6aa93fb7b63151b984
---
# Source: slurm/templates/secrets/secret-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/name: slurm-secret-job
    app.kubernetes.io/instance: slurm-secret-job
    helm.sh/chart: slurm
    app.kubernetes.io/part-of: slurm
    app.kubernetes.io/managed-by: Helm
  name: slurm-secret-job
spec:
  template:
    spec:
      shareProcessNamespace: true
      dnsConfig:
        searches:
          - slurm-compute.tenant-sta-asenetar5-1.svc.cluster.local
          - slurm-controller.tenant-sta-asenetar5-1.svc.cluster.local
      enableServiceLinks: false
      initContainers:
      # The initcontainer is needed mostly to persist etc from the image in an emptyDir
      # This way we can modify /etc/passwd and similar via a SSSD sidecar
      - name: init
        image: "registry.gitlab.com/coreweave/sunk/controller-extras:v1.0.0"
        command: ["/usr/share/sunk/bin/init.sh"]
        volumeMounts:
        - name: etc
          mountPath: /runtime/etc
        - name: run
          mountPath: /run
        - name: munge
          mountPath: /munge/munge.secret.key
          subPath: munge.key
          readOnly: true
      containers:
      - name: munged
        image: "registry.gitlab.com/coreweave/sunk/controller:v1.0.0"
        command: ["munged", "-F"]
        resources:
          limits:
            cpu: 1
            memory: 2Gi
        volumeMounts:
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
        lifecycle:
          preStop:
            exec:
              # Give other containers a chance to do what they need to do to stop beore munge dies
              command: ["sleep", "10"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 96 # Assume that munge is 96 always
          runAsGroup: 96
      - name: job
        image: "registry.gitlab.com/coreweave/sunk/controller-extras:v1.0.0"
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 200m
            memory: 100Mi
        command: ["/usr/share/sunk/bin/create-syncer-secret.sh", "slurm"]
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: run
          mountPath: /run
        - name: etc
          mountPath: /etc
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      - name: sssd-pipes
        emptyDir:
          medium: Memory
      - name: sssd-cache
        emptyDir:
          medium: Memory
      - name: run
        emptyDir:
          medium: Memory
      - name: etc
        emptyDir:
          medium: Memory
      - name: munge
        secret:
          secretName: slurm-munge-secret
      - name: krb5-conf
        configMap:
          name: slurm-krb5-conf
          defaultMode: 0600
      - name: sssd-conf
        configMap:
          name: slurm-sssd-conf
          defaultMode: 0600
      - name: sudoers-groups
        secret:
          secretName: slurm-slurm-sudoers
          defaultMode: 0400
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - LGA1
              - key: cpu.coreweave.cloud/family
                operator: In
                values:
                  - epyc
      serviceAccountName: slurm-secret-job
      restartPolicy: Never
  backoffLimit: 4

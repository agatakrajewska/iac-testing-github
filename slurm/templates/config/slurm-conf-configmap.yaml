apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-slurm-conf
  labels:
    {{- include "slurm.labels" . | nindent 4 }}
data:
  slurm.conf: |
    ClusterName={{ .Release.Name }}
    SlurmctldHost={{ .Release.Name }}-controller
    SlurmctldParameters={{- join "," (concat (list "cloud_dns") .Values.slurmConfig.slurmCtld.additionalParameters) }}
    CommunicationParameters=NoAddrCache,KeepAliveTime=60,keepaliveinterval=10,keepaliveprobes=3
    SlurmctldTimeout={{ .Values.slurmConfig.slurmCtld.timeout }}
    SlurmdTimeout={{ .Values.slurmConfig.slurmd.timeout }}
    SuspendTime={{ .Values.slurmConfig.slurmd.suspendTime }}
    PrivateData=cloud
    MpiDefault=pmi2 # pmix can be used for Ubuntu 22
    ProctrackType={{ .Values.slurmConfig.slurmCtld.procTrackType }}
    ReturnToService=2
    SlurmctldPidFile=/var/run/slurmctld.pid
    SlurmctldPort=6817
    SlurmdPidFile=/var/run/slurmd.pid
    SlurmdPort=6818
    SlurmdSpoolDir=/var/spool/slurmd
    SlurmUser=slurm
    StateSaveLocation=/var/spool/slurmctld/save
    SwitchType=switch/none
    TaskPlugin={{ .Values.slurmConfig.slurmCtld.taskPlugin }}
    #TaskPluginParam=None
    TopologyPlugin=topology/tree
    TopologyParam=TopoOptional
    InactiveLimit={{ .Values.slurmConfig.inactiveLimit }}
    KillWait={{ .Values.slurmConfig.killWait }}
    MinJobAge=300
    Waittime={{ .Values.slurmConfig.waitTime }}
    SchedulerType=sched/backfill
    SelectType=select/cons_tres
    SelectTypeParameters={{ .Values.slurmConfig.selectTypeParameters }}
    {{- if .Values.slurmConfig.slurmCtld.jobSubmitPlugins }}
    JobSubmitPlugins={{ .Values.slurmConfig.slurmCtld.jobSubmitPlugins }}
    {{- end }}
    JobCompType=jobcomp/none
    SlurmctldDebug=verbose
    SlurmctldLogFile=/dev/null
    SlurmSchedLogLevel=1
    SlurmSchedLogFile=/dev/null
    SlurmdDebug=verbose
    SlurmdLogFile=/dev/null
    TreeWidth=65533
    GresTypes=gpu
    MaxNodeCount=1024
    DefMemPerCPU={{ .Values.slurmConfig.defMemPerCPU }}
    UnkillableStepTimeout=900
    RebootProgram=/usr/share/sunk/bin/reboot.sh

    {{- if .Values.accounting.enabled }}
    JobAcctGatherType=jobacct_gather/linux
    JobAcctGatherFrequency=30
    AccountingStorageEnforce={{ .Values.slurmConfig.slurmCtld.accountingStorageEnforce }}
    AccountingStorageTRES=gres/gpu
    AccountingStorageType=accounting_storage/slurmdbd
    {{- if .Values.accounting.external.enabled }}
    AccountingStorageHost={{ .Values.accounting.external.host }}
    AccountingStoragePort={{ .Values.accounting.external.port }}
    AccountingStorageUser={{ .Values.accounting.external.user }}
    {{- else }}
    AccountingStorageHost={{ .Release.Name }}-accounting
    AccountingStorageUser={{ .Values.mysql.auth.username }}
    AccountingStoragePort=6819
    {{- end }}
    {{- else }}
    JobAcctGatherFrequency=30
    JobAcctGatherType=jobacct_gather/none
    AccountingStorageType=accounting_storage/none
    {{- end }}

    {{- if .Values.slurmConfig.slurmd.prologConfigMap }}
    Prolog=/usr/share/sunk/bin/prolog.sh
    {{- end }}
    {{- if .Values.slurmConfig.slurmd.epilogConfigMap }}
    Epilog=/usr/share/sunk/bin/epilog.sh
    {{- end }}
    {{- if .Values.compute.ssh.enabled }}
    PrologFlags=Alloc,Serial,Contain
    {{- else }}
    PrologFlags=Alloc,Serial
    {{- end }}

    AuthAltTypes=auth/jwt
    AuthAltParameters=jwt_key={{ include "slurm.jwt.path" . }}

    {{- .Values.slurmConfig.extraConfig | nindent 4 -}}

    {{ range $name, $v := .Values.compute.nodes }}
    {{- if $v.enabled -}}
    Nodeset={{ $name }} Feature={{ $name }}
    {{- if $.Values.compute.autoPartition.enabled }}
    PartitionName={{ $name }} Nodes={{ $name }} Default=no MaxTime=INFINITE State=UP
    {{- end }}
    {{ if not $.Values.compute.dynamic -}}
    include {{ $.Release.Name }}-{{ $name }}-nodes.conf
    {{- end }}
    {{- end }}
    {{- end }}
    {{ .Values.compute.partitions | nindent 4 }}

  gres.conf: |
    {{ if .Values.compute.dynamic -}}
    AutoDetect=nvml
    {{ else -}}
    AutoDetect=off
    {{ range $name, $v := .Values.compute.nodes -}}
      {{- if $v.definitions -}}
        {{- $dest := ($.Files.Get "compute-defs/base.yaml" | fromYaml).base}}
        {{- $def_files := $.Files.Glob "compute-defs/**.yaml"}}
        {{- $defs := deepCopy $.Values.compute.nodes }}
        {{- /* accumulate all of the file values into a single object*/}}
        {{-  range $path, $_ := $def_files -}}
          {{- $defs = merge $defs ( $.Files.Get $path | fromYaml) }}
        {{- end -}}
        {{- if gt (len $v.definitions) 0 -}}
          {{- range $v.definitions -}}
              {{- if not (hasKey $defs .) -}}
                {{- fail (printf "compute node %s has an undefined def %s" $name .) -}}
              {{- end -}}
              {{- $dest = (include "common.general.update" (dict "base" $dest "src" (get $defs .) "forceAffinityUpdate" true)) | fromYaml }}
              {{- $dest = (include "retemplate" (dict "value" $dest "context" $)) | fromYaml}}
          {{- end -}}
        {{- end -}}
        {{- $v = (include "common.general.update" (dict "base" $dest "src" $v "forceAffinityUpdate" true)) | fromYaml}}
        {{- $v = (include "retemplate" (dict "value" $v "context" $)) | fromYaml}}
        {{- $v = omit $v "definitions" }}
      {{- end -}}
    {{- if and $v.enabled (index $v.resources.requests "sunk.coreweave.com/accelerator") -}}
    {{- $resource := printf "Name=gpu Type=%s File=/dev/nvidia[0-%d]" (regexReplaceAllLiteral "(:[0-9])$" $v.gresGpu "") (sub (index $v.resources.requests "sunk.coreweave.com/accelerator") 1) -}}
    {{- range $subnet := until 241 }}
    NodeName={{ $.Release.Name }}-{{ $name }}-{{ printf "%03d" $subnet }}-[00-99] {{ $resource }}
    {{- end }}
    {{- end -}}
    {{- end -}}
    {{- end -}}

  {{- if .Values.compute.pyxis.enabled }}
  plugstack.conf: |
    required /usr/local/lib/slurm/spank_pyxis.so
  {{- end }}
  topology.conf: |
    {{ range $name, $v := .Values.compute.nodes -}}
      {{- if $v.enabled -}}
      include {{ $.Release.Name }}-{{ $name }}-topology.conf
      {{ end -}}
    {{ end -}}
